from typing import TYPE_CHECKING, List, Literal, Optional, Union
from parallellm.core.ask import Askable
from parallellm.core.cast.fix_docs import cast_documents
from parallellm.core.exception import NotAvailable
from parallellm.core.hash import compute_hash
from parallellm.core.identity import LLMIdentity
from parallellm.core.msg.state import MessageState
from parallellm.core.response import (
    LLMResponse,
    ReadyLLMResponse,
)
from parallellm.logging.dash_logger import HashStatus
from parallellm.types import (
    AskParameters,
    CallIdentifier,
    CommonQueryParameters,
    LLMDocument,
    ServerTool,
)

if TYPE_CHECKING:
    from parallellm.core.agent.orchestrator import AgentOrchestrator


class AgentContext(Askable):
    """Context manager for Agent lifecycle (default context)"""

    def __init__(
        self,
        agent_name: str,
        batch_manager: "AgentOrchestrator",
        *,
        ask_params: Optional[AskParameters] = None,
        ignore_cache: bool = False,
    ):
        self.agent_name = agent_name
        self._bm = batch_manager

        self._anonymous_counter = 0

        self.ask_params = ask_params or {}
        self.ignore_cache = ignore_cache

        self._msg_state: Optional[MessageState] = None
        "MessageState for this agent. Some pipelines won't use this (so it will be None)."
        self._persist_msg_state: bool = True

    def __enter__(self):
        # No setup needed
        return self

    def __exit__(self, exc_type, exc_value, traceback):
        # Save message state
        if self._msg_state is not None and self._persist_msg_state:
            self._try_persist_msg_state(self._msg_state)

        if exc_type in (NotAvailable,):
            return True
        if self._bm.strategy == "batch" and exc_type in (NotAvailable,):
            # swallow NotAvailable errors only in batch mode
            return True
        return False

    def print(self, *args, **kwargs):
        """
        Print to console above the dashboard output.
        This ensures proper display ordering when the dashboard is active.
        """
        print(*args, **kwargs)

    @property
    def my_metadata(self) -> dict:
        """
        Metadata for this agent.
        Backed by the AgentOrchestrator's FileManager.
        """
        key = self.agent_name if self.agent_name is not None else ""
        return self._bm._fm.metadata["agents"].setdefault(
            key,
            {},
        )

    def ask_llm(
        self,
        documents: Union[LLMDocument, List[LLMDocument], MessageState],
        *additional_documents: LLMDocument,
        instructions: Optional[str] = None,
        llm: Union[LLMIdentity, str, None] = None,
        salt: Optional[str] = None,
        hash_by: Optional[List[Literal["llm"]]] = None,
        text_format: Optional[str] = None,
        tools: Optional[list[Union[dict, ServerTool]]] = None,
        tag: Optional[str] = None,
        save_input: bool = False,
        **kwargs,
    ) -> LLMResponse:
        # load ask_params defaults
        for k, v in self.ask_params.items():
            if k == "hash_by" and hash_by is None:
                hash_by = v

        if llm is None:
            llm = self._bm._provider.get_default_llm_identity()
        elif isinstance(llm, str):
            llm = LLMIdentity(llm)

        seq_id = self._anonymous_counter
        self._anonymous_counter += 1

        if isinstance(documents, MessageState):
            documents = documents.cast_documents() + list(additional_documents)
        else:
            documents = cast_documents(documents, list(additional_documents))

        # Compute salt
        salt_terms: list[str] = []
        if salt is not None:
            salt_terms.append(salt)
        if hash_by is not None:
            for term in hash_by:
                if term == "llm":
                    if llm is not None:
                        salt_terms.append(llm.identity)
                    else:
                        salt_terms.append(self._bm._provider.provider_type)
        hashed = compute_hash(instructions, documents + salt_terms)

        if save_input:
            msg_hashes = [compute_hash(None, [msg]) for msg in documents]
            self._bm._backend._get_datastore().store_doc_hash(
                hashed,
                instructions=instructions,
                documents=documents,
                salt_terms=salt_terms,
                msg_hashes=msg_hashes,
            )

        call_id: CallIdentifier = {
            "agent_name": self.agent_name,
            "doc_hash": hashed,
            "seq_id": seq_id,
            "session_id": self._bm.get_session_counter(),
            "meta": {
                "provider_type": self._bm._provider.provider_type,
                "tag": tag,
            },
        }

        # Cache using datastore
        cached = None if self.ignore_cache else self._bm._backend.retrieve(call_id)
        if cached is not None:
            # if self._bm.strategy != "batch":
            self.update_hash_status(hashed, HashStatus.CACHED)
            return ReadyLLMResponse(
                call_id=call_id,
                pr=cached,
            )

        if not self._bm._provider.is_compatible(llm.provider):
            raise ValueError(
                f"LLM {llm.identity} is not compatible"
                + f" with provider {self._bm._provider.provider_type}"
            )

        params: CommonQueryParameters = {
            "instructions": instructions,
            "documents": documents,
            "llm": llm,
            "text_format": text_format,
            "tools": tools,
        }

        return self._bm._backend.submit_query(
            self._bm._provider,
            params,
            call_id=call_id,
            **kwargs,
        )

    def update_hash_status(self, hash_value: str, status: HashStatus):
        # No-op
        pass

    def get_msg_state(self, persist=True) -> MessageState:
        """
        Get the current MessageState for this agent.

        :param persist: Whether to persist the MessageState upon exit.
            (This lets you save and resume conversations.
            If False, responses still get cached in the backend,
            but they won't appear in MessageState.)
        :returns: The current message state.
        """
        if self._msg_state is None:
            self._msg_state = self._bm.get_msg_state(self)
            self._persist_msg_state = persist

        return self._msg_state

    def _try_persist_msg_state(self, msg_state):
        self._bm.save_msg_state(
            self,
            msg_state,
        )


class AgentDashboardContext(AgentContext):
    """Context manager for the hash status dashboard"""

    def __init__(
        self,
        agent_name: str,
        batch_manager: "AgentOrchestrator",
        *,
        log_k: int = 10,
        ask_params: Optional[AskParameters] = None,
        ignore_cache: bool = False,
    ):
        super().__init__(
            agent_name, batch_manager, ask_params=ask_params, ignore_cache=ignore_cache
        )
        self._was_displaying = False
        self._bm._dashlog.k = log_k

    @property
    def _dashlog(self):
        return self._bm._dashlog

    def __enter__(self):
        # Store current display state and enable display
        self._was_displaying = self._dashlog.display
        self._dashlog.set_display(True)
        return super().__enter__()

    def __exit__(self, exc_type, exc_val, exc_tb):
        self._dashlog._update_console()

        self._dashlog.finalize_line()
        self._dashlog.set_display(self._was_displaying)
        print()
        return super().__exit__(exc_type, exc_val, exc_tb)

    def print(self, *args, **kwargs):
        """
        Print to console above the dashboard output.
        This ensures proper display ordering when the dashboard is active.
        """
        self._dashlog.print(*args, **kwargs)

    def update_hash_status(self, hash_value: str, status: HashStatus):
        """
        Update the status of a hash in the logger

        Args:
            hash_value: The hash value to update
            status: New status - one of 'C' (cached), '↗' (sent), '↘' (received), '✓' (stored)
        """
        self._dashlog.update_hash(hash_value, status)
