"""
In comparison to `simplest_tool.py`, this example demonstrates how MessageState
to conveniently store conversation history.
"""

import logging

from pydantic import BaseModel
from parallellm.core.gateway import ParalleLLM
from parallellm.types import FunctionCallOutput
from dotenv import load_dotenv

load_dotenv()


class MyModel(BaseModel):
    final_answer: str


tools = [
    {
        "type": "function",
        "name": "count_files",
        "description": "Count the number of files in a directory.",
        "parameters": {
            "type": "object",
            "properties": {
                "directory": {
                    "type": "string",
                    "description": "The path to the directory to count files in.",
                },
            },
            "required": ["directory"],
        },
    }
]


def ls_tool(directory) -> str:
    return f"There are 4 files in {directory}."


with ParalleLLM.resume_directory(
    ".pllm/simplest-tool",
    provider="openai",
    strategy="sync",
    log_level=logging.DEBUG,
    # ignore_cache=True,
) as pllm:
    with pllm.agent(dashboard=True) as dash:
        # Tools
        convo = dash.get_msg_state(persist=False)
        resp = convo.ask_llm(
            "How many files are in '~/examples'? Give the final answer in words.",
            hash_by=["llm"],
            tools=tools,
            # llm="gpt-4o"
        )

        tool_calls = resp.resolve_function_calls()
        assert len(tool_calls) == 1
        assert tool_calls[0].name == "count_files"

        computed_tool_output = FunctionCallOutput(
            name=tool_calls[0].name,
            content=ls_tool(tool_calls[0].args),
            call_id=tool_calls[0].call_id,
        )
        convo.append(computed_tool_output)

        resp = convo.ask_llm(hash_by=["llm"])
        dash.print(convo)
